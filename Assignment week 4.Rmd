---
title: "Practical Machine Learning Course Project"
subtitle: "Assignment 4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Aim
The aim of this project is to create a report describing how I built a classification model of human activity recognition, how I used cross-validation, what I think the expected out of sample error is, and why I made the choices I did.


### Background
*The data and the background info text is from [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har).*

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behaviour, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

#### Activity Model
Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (**Class A**), throwing the elbows to the front (**Class B**), lifting the dumbbell only halfway (**Class C**), lowering the dumbbell only halfway (**Class D**) and throwing the hips to the front (**Class E**).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience.






### Data
The data used for this project are available using the following links:

Training data: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

Test data: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)







#### Setup - Getting and Preparing the Data Sets
```{r preliminaries, results='hide', message=FALSE, warning=FALSE}
packages <- c("caret", "randomForest", "parallel","doParallel", "knitr")
sapply(packages, require, character.only = TRUE, quietly = TRUE)
```

Retrieve and load the data sets.
```{r}
urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(urlTrain), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(urlTest), na.strings=c("NA","#DIV/0!",""))
```

Let us check the dimensions of the data, to get an idea of what we are dealing with.

```{r}
dim(training)
```

We find that the training data contains 19622 rows of data, which should be plenty to perform cross-validation.






#### Preparing the Data Sets
By viewing the data we notice that some of the columns in the data sets contain irrelevent information (the first seven coloumns as it turns out: "X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window"). Also, many of the variables appears to hold only NA values.

To avoid unnecessary clutter we start off by removing these columns.

```{r}
# Update data sets to exclude variables with only NA values
training <- training[,colSums(is.na(training)) == 0]
testing <- testing[,colSums(is.na(testing)) == 0]

# Remove variables that are irrelevant to the prediction
training <- training[, -c(1:7)]
testing <- testing[, -c(1:7)]
```

As a final attempt to clean the data, we remove near zero variance variables (if there are any).

```{r}
# Remove near zero variance variables
training <- training[, nearZeroVar(training, saveMetrics = T)$nzv == F]
testing <- testing[, nearZeroVar(testing, saveMetrics = T)$nzv == F]
```







### A Note on Model Selection
I have applied the train function in caret using several different methods, and five-fold cross-validation. The result with respect to accuracy is as follows:

1) "**rf**" (random forest) accuracy was 0.9946982
2) "**rpart**" (recursive partitioning and regression trees) accuracy was 0.7402121
3) "**gbm**" (stochastic gradient boosting) accuracy was 0.9647227
4) "**xgbTree**" (extreme gradient boosting) accuracy was 0.9963295

I also tried improving the accuracy by combining the rf model and the xgbTree model, which result in an accuracy of 0.9973491. Using this approach comes at a price, however, as cross-validation with the caret train function is quite computationally intensive.

As a result of these considerations, I settled on simply using randomForest, with only a single cross-validation sample. This approach is **much** less power hungry, and the accuracy is nearly as good as the best of the above mentioned approaches, and I find no need to improve the accuracy further.






#### Cross-Validation
We split the training data into two groups (p = 0.75), and use the larger group for training, and the smaller for cross-validation.

```{r}
set.seed(42)

inTrain <- createDataPartition(training$classe, p=0.75, list=F)

trainingCV <- training[inTrain,]
testingCV <- training[-inTrain,]
```

To speed up the analysis, we now take advantages of multiple cores.

```{r}
clustFit <- makeCluster(detectCores() - 1)
registerDoParallel(clustFit)

model_RF <- randomForest(classe ~ .,
                         data=trainingCV,
                         method="class",
                         ntree = 1000)

stopCluster(clustFit)
```






### Evaluating the model
To estimate the accuracy, and the out-of-sample error, we test the model against the validation data.

```{r}
prediction_RF <- predict(model_RF, testingCV)
confusionMatrix(prediction_RF, testingCV$classe)
```

From the confusionmatrix, we get an **accurary of 99.63%**. This corresponds to an **out-of-sample error of only 0.37%**.






### Final Prediction
The result of predicting the activity from the test data using model_RF is:

```{r}
FinalPrediction <- predict(model_RF, testing)
kable(t(data.frame(FinalPrediction)))
```